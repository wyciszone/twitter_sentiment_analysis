{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNVwKcUQoSRCWk2yLrJdtky"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas scikit-learn nltk datasets xgboost"
      ],
      "metadata": {
        "id": "BUtrDSJcZaFm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a327f30-35c8-46fc-a425-7d5eb4040035"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.2.0)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.1.3)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.11)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.27.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2024.12.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5sge_zrHNKZX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a964a7f2-b60d-4b45-ba06-ba868996a599"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import string\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from datasets import load_dataset\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# data\n",
        "dataset = load_dataset('carblacac/twitter-sentiment-analysis', trust_remote_code=True)\n",
        "\n",
        "df_train = pd.DataFrame(dataset['train'])\n",
        "df_test = pd.DataFrame(dataset['test'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "#lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "cS-h0vNiNoE1"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# classifiers\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import RidgeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from xgboost import XGBClassifier\n",
        "classifiers = {\n",
        "    'Ridge Classifier': RidgeClassifier(),\n",
        "    'Logistic Regression': LogisticRegression(max_iter=1000),\n",
        "    'Naive Bayes': MultinomialNB(),\n",
        "    'Random Forest': RandomForestClassifier(n_estimators=100),\n",
        "    'KNN': KNeighborsClassifier(n_neighbors=5)\n",
        "}\n",
        "\n",
        "# Store results\n",
        "results = {}"
      ],
      "metadata": {
        "id": "FYfwxGneti3f"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocessing\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()  # lowercase\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)  # del urls\n",
        "    text = re.sub(r'\\@\\w+|\\#', '', text)  # del mentions and hashtags\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))  # del punctuation\n",
        "    tokens = word_tokenize(text)  # tokenize\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]  # del stopwords & lemmatize\n",
        "    return ' '.join(tokens)"
      ],
      "metadata": {
        "id": "xvTnxesENxZ4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# apply\n",
        "df_train['clean_text'] = df_train['text'].apply(preprocess_text)\n",
        "df_test['clean_text'] = df_test['text'].apply(preprocess_text)"
      ],
      "metadata": {
        "id": "EGQhl_c3N_tW"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name, clf in classifiers.items():\n",
        "    print(f\"Training with {name}...\")\n",
        "\n",
        "    model = Pipeline([\n",
        "        ('vectorizer', TfidfVectorizer()),  # TF-IDF feature extraction\n",
        "        ('classifier', clf)  # Classifier\n",
        "    ])\n",
        "\n",
        "    # train\n",
        "    model.fit(df_train['clean_text'], df_train['feeling'])\n",
        "\n",
        "    y_pred = model.predict(df_test['clean_text'])\n",
        "\n",
        "    accuracy = accuracy_score(df_test['feeling'], y_pred)\n",
        "    results[name] = {\n",
        "        'accuracy': accuracy,\n",
        "        'report': classification_report(df_test['feeling'], y_pred)\n",
        "    }\n",
        "    print(f\"Accuracy with {name}: {accuracy}\")\n",
        "    print(classification_report(df_test['feeling'], y_pred))\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "id": "WAqly-1WOJx_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "784d430a-6b68-4b95-f606-959c079b0674"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with Ridge Classifier...\n",
            "Accuracy with Ridge Classifier: 0.7559114810155166\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.74      0.75     30969\n",
            "           1       0.75      0.77      0.76     31029\n",
            "\n",
            "    accuracy                           0.76     61998\n",
            "   macro avg       0.76      0.76      0.76     61998\n",
            "weighted avg       0.76      0.76      0.76     61998\n",
            "\n",
            "\n",
            "\n",
            "Training with Logistic Regression...\n",
            "Accuracy with Logistic Regression: 0.7671053904964676\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.75      0.76     30969\n",
            "           1       0.76      0.78      0.77     31029\n",
            "\n",
            "    accuracy                           0.77     61998\n",
            "   macro avg       0.77      0.77      0.77     61998\n",
            "weighted avg       0.77      0.77      0.77     61998\n",
            "\n",
            "\n",
            "\n",
            "Training with Naive Bayes...\n",
            "Accuracy with Naive Bayes: 0.7515081131649408\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.78      0.76     30969\n",
            "           1       0.77      0.73      0.75     31029\n",
            "\n",
            "    accuracy                           0.75     61998\n",
            "   macro avg       0.75      0.75      0.75     61998\n",
            "weighted avg       0.75      0.75      0.75     61998\n",
            "\n",
            "\n",
            "\n",
            "Training with Random Forest...\n",
            "Accuracy with Random Forest: 0.7535404367882835\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.78      0.76     30969\n",
            "           1       0.77      0.73      0.75     31029\n",
            "\n",
            "    accuracy                           0.75     61998\n",
            "   macro avg       0.75      0.75      0.75     61998\n",
            "weighted avg       0.75      0.75      0.75     61998\n",
            "\n",
            "\n",
            "\n",
            "Training with KNN...\n",
            "Accuracy with KNN: 0.591986838285106\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.59      0.62      0.60     30969\n",
            "           1       0.60      0.57      0.58     31029\n",
            "\n",
            "    accuracy                           0.59     61998\n",
            "   macro avg       0.59      0.59      0.59     61998\n",
            "weighted avg       0.59      0.59      0.59     61998\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# results\n",
        "print(\"\\nFinal Accuracy Comparison:\")\n",
        "for name, metrics in results.items():\n",
        "    print(f\"{name}: {metrics['accuracy']:.4f}\")"
      ],
      "metadata": {
        "id": "RN0Cx0vpOivj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb5780cd-4663-45ad-d189-1c8410597190"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Accuracy Comparison:\n",
            "Ridge Classifier: 0.7559\n",
            "Logistic Regression: 0.7671\n",
            "Naive Bayes: 0.7515\n",
            "Random Forest: 0.7535\n",
            "KNN: 0.5920\n"
          ]
        }
      ]
    }
  ]
}